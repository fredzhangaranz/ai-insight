# AI Validation Strategy for InsightGen

This document outlines the multi-layered strategy for validating the output of the InsightGen AI. The primary goal is to ensure that all AI-generated content is both **clinically relevant** and **technically correct**.

We treat the AI not as an infallible oracle, but as a powerful new team member that requires guidance, feedback, and verification.

---

## 1. Clinical Relevance Validation

This addresses the question: _"Are the AI's suggestions (questions and insights) actually useful and meaningful to a clinician?"_

The core strategy is to establish a **human-in-the-loop feedback system** that leverages the expertise of our end-users.

### A. User Feedback Mechanisms (UI-Driven)

- **Implicit Feedback (Usage Analytics):** Track which AI-generated questions are most frequently selected by users. Over time, this data will reveal which analytical paths are most valuable.
- **Explicit Feedback (Rating System):** Implement a simple "thumbs up/down" or star rating system for each generated insight or question. This provides direct, immediate feedback on the relevance and quality of the AI's suggestions.
- **User Refinement and Correction:** Allow users to edit or refine an AI-generated question. Capturing these modifications provides invaluable data on the specific ways the AI's understanding differs from the clinician's intent.

### B. Seeding the AI with Expert Knowledge

- **"Gold Standard" Question Set:** Collaborate with clinical experts to create a manually curated list of high-value analytical questions for each form. This set can be used as a benchmark for evaluating the AI and to "prime" the AI's prompts for better results.
- **Enhanced Prompt Context:** Improve the AI's system prompt by providing more context about the clinical goals. For instance, instead of just providing the schema, we can add: `"Your goal is to identify trends in healing, flag potential infection risks, and analyze treatment effectiveness."`

---

## 2. Technical Correctness Validation

This addresses the question: _"Is the SQL query generated by the AI safe, performant, and accurate?"_

The core strategy is **"trust, but verify,"** combining transparency with automated checks.

### A. Transparency (The "Glass Box" Approach)

- **Show the Work:** The current UI, which displays the generated SQL query alongside the chart and the raw data table, is the most critical component of technical validation. It allows technical users to manually inspect and verify the query logic, building trust in the system. **This feature is essential and must be maintained.**

### B. Automated Guardrails

- **Security Checks:**

  - **Read-Only Enforcement:** The `execute-query` endpoint must validate that the query is read-only. We already check if it starts with `SELECT` or `WITH`, and we can enhance this to explicitly forbid keywords like `DELETE`, `UPDATE`, `DROP`, etc.
  - **Parameterized Queries:** Continue using parameterized queries (e.g., for `@patientId`) to completely prevent SQL injection vulnerabilities. This is non-negotiable.

- **Automated Query Testing:**

  1.  **Create a Test Database:** Use a small, static, and well-understood subset of the production database for testing.
  2.  **Define Test Cases:** For each "gold standard" question, write the 100% correct SQL query manually.
  3.  **Establish a Baseline:** Execute the manual query against the test database and store the result as the "expected output."
  4.  **Run and Compare:** Ask the AI to generate a query for the same question. Execute the AI's query against the test database.
  5.  **Assert Equality:** Automatically compare the AI's result with the expected output. A match provides high confidence in the AI's ability to generate that type of query.

- **Performance Monitoring (Future Consideration):**

  - Before execution, we can ask the database for the query's `EXPLAIN PLAN`. If the estimated cost is excessively high, we can reject the query to prevent it from overwhelming the database.

- **Data Plausibility Checks:**
  - After receiving data, run simple sanity checks. For example, if a query for "wound area" returns negative numbers, we know the query logic is flawed, even if the syntax was correct.

---

By combining clinical feedback with technical safeguards, we can build an AI-powered analytics tool that is not only powerful but also trustworthy and reliable.
